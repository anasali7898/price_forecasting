import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)
from sklearn.metrics import mean_squared_error, r2_score


## //  reading dataset
data_train = pd.read_csv("./dataset/train.csv",sep="|")
data_items = pd.read_csv("./dataset/items.csv",sep="|")
data_new = pd.merge( data_train, data_items, how='left', on='pid' )


X_train, X_val, y_train, y_val = train_test_split( data_new.drop(columns=['lineID','pid',"price"]), data_new[["price"]], test_size=0.2 )

print("Training Data X : ",X_train.shape, "Validation Data X : ", X_val.shape, "Training Data Y : ", y_train.shape, "Validation Data Y", y_val.shape)


## for col in X_train.columns:
##    print( "Column: ", col , "  | Count of Missing Value: ", X_train[col].isna().sum() ,"  |  Percentage of Missing Values", (X_train[col].isna().sum()/X_train.shape[0])*100, "\n ------------" )


# Drop columns with more than 50% missing values
X_train.drop(columns=['campaignIndex'],inplace=True)
X_val.drop(columns=['campaignIndex'],inplace=True)

print("post removal")
print("Training Data X : ",X_train.shape, "Validation Data X : ", X_val.shape, "Training Data Y : ", y_train.shape, "Validation Data Y", y_val.shape)


# Change data type of category column into integer and skip missing values
def convert_to_int(x):
    try:
        return int(x)
    except:
        return np.nan

print(X_train.columns);

X_train['category'] = X_train['category'].apply(convert_to_int)
X_val['category'] = X_val['category'].apply(convert_to_int)



# Print catgorical columns and count of unique values in each column
cat_cols = ['group','unit','pharmForm','salesIndex']
cat_num_cols =  ['manufacturer','adFlag','availability', "genericProduct", 'category']
##for col in cat_cols+cat_num_cols:
  ##  print( "Column: ", col , "  | Count of Unique Values: ", X_train[col].nunique() , "\n ------------" )
    # plot histogram
    # fig = px.histogram( X_train, x=col, title=col )
    # fig.show()




def convert_content_column_into_numbers(item):
    try:
        return float(item)
    except:
        # extract numbers seperated by alphabet (using regex)
        import re
        item = re.findall(r'\d+\.?\d*', item)
        # multiply the numbers to get the final number
        final_number = 1
        for i in item:
            final_number = final_number*float(i)
        return final_number


# item = 'L  150'
# convert_content_column_into_numbers(item)

X_train['content'] = X_train['content'].apply(convert_content_column_into_numbers)
X_val['content'] = X_val['content'].apply(convert_content_column_into_numbers)

# for item in X_train['content'].unique():
#     print(item, "   |   ", convert_content_column_into_numbers(item))

print(X_train)
print(X_train.shape)


print(X_train[cat_cols+cat_num_cols].isna().sum())


# Impute missing values in cat_num_cols categorical columns


imputer = SimpleImputer( strategy='constant', fill_value=-1 )

X_train[cat_num_cols] = imputer.fit_transform( X_train[cat_num_cols] )
X_val[cat_num_cols] = imputer.transform( X_val[cat_num_cols] )
# convert the data type of cat_num_cols columns into integer
X_train[cat_num_cols] = X_train[cat_num_cols].astype(int)
X_val[cat_num_cols] = X_val[cat_num_cols].astype(int)

# Impute missing values in cat_cols categorical columns with mode value
imputer = SimpleImputer( strategy='most_frequent' )
X_train[cat_cols] = imputer.fit_transform( X_train[cat_cols] )
X_val[cat_cols] = imputer.transform( X_val[cat_cols] )


for col in cat_cols:
    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
    X_train[col] = encoder.fit_transform( X_train[[col]] )
    X_val[col] = encoder.transform( X_val[[col]] )
    pickle.dump( encoder, open( col+"_encoder.pkl", "wb" ) )

X_train[cat_cols+cat_num_cols] = X_train[cat_cols+cat_num_cols].astype(int)
X_val[cat_cols+cat_num_cols] = X_val[cat_cols+cat_num_cols].astype(int)

print(X_train)

X_train.isna().sum()
# y_val= y_val.fillna(0)
# y_val.isna().sum()

model = CatBoostRegressor( iterations=100, cat_features=cat_cols+cat_num_cols,
                           loss_function='RMSE', custom_metric=['RMSE',"R2"], random_seed=10 )
model.fit( X_train, y_train, eval_set=(X_val,y_val), plot=False )

X_train_catboost = X_train.copy()
X_val_catboost = X_val.copy()


print(X_train_catboost)



def objective(trial):
    params = {
        'min_child_samples' : trial.suggest_int('min_child_samples', 1, 300),
        'iterations': trial.suggest_int('iterations', 50, 300),
        'depth': trial.suggest_int('depth', 4, 10),
        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.5),
        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.0, 1.0),
        'cat_features': cat_cols+cat_num_cols,
        'loss_function': 'RMSE',
        'custom_metric': ['RMSE',"R2"],
        'random_seed': 10,
        'use_best_model': True,
        'silent' : True
    }

    model = CatBoostRegressor(**params)
    model.fit( X_train, y_train, eval_set=(X_val,y_val) )

    y_pred = model.predict(X_val)
    r2 = np.sqrt(mean_squared_error(y_val, y_pred))
    return r2

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=2)
study.best_params

# Train the model with best parameters
model = CatBoostRegressor( iterations=study.best_params['iterations'], learning_rate=study.best_params['learning_rate'],
                            depth=study.best_params['depth'], min_child_samples=study.best_params['min_child_samples'],
                            bagging_temperature=study.best_params['bagging_temperature'],
                            cat_features=cat_cols+cat_num_cols, loss_function='RMSE',
                            custom_metric=['RMSE',"R2"], random_seed=10 )
model.fit( X_train, y_train, eval_set=(X_val,y_val), plot=False )


# Feature Importance
feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,X_train.columns)), columns=['Value','Feature'])
plt.figure(figsize=(20, 10))
sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False))
plt.title('CatBoostRegressor Features (avg over folds)')
plt.tight_layout()
plt.show()

# Save the model
pickle.dump( model, open( "Catboost_model.pkl", "wb" ) )

# Load the model
model = pickle.load( open( "Catboost_model.pkl", "rb" ) )

# Calculate rsme, normalized rmse and r2 score on both train and validation data
from sklearn.metrics import mean_squared_error, r2_score
print('----------------------')
y_pred = model.predict(X_train)
print("Train RMSE: ", np.sqrt(mean_squared_error(y_train, y_pred)))
print("Train Normalized RMSE: ", np.sqrt(mean_squared_error(y_train, y_pred))/(y_train.max()-y_train.min()))
print("Train R2 Score: ", r2_score(y_train, y_pred))
print("Mean Absolute Percentage Error:", mean_absolute_percentage_error(y_train, y_pred))
print('-----')
y_pred = model.predict(X_val)
print("Validation RMSE: ", np.sqrt(mean_squared_error(y_val, y_pred)))
print("Validation Normalized RMSE: ", np.sqrt(mean_squared_error(y_val, y_pred))/(y_val.max()-y_val.min()))
print("Validation R2 Score: ", r2_score(y_val, y_pred))
print("Mean Absolute Percentage Error:", mean_absolute_percentage_error(y_val, y_pred))
print('----------------------')

